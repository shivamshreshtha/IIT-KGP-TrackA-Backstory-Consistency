ğŸ“˜ Backstoryâ€“Narrative Consistency Verification System
Track A: Systems Reasoning with NLP & Generative AI
1. ğŸ§© Problem Statement

Given:

a full-length novel (100k+ words), and

a hypothetical backstory for a central character,

the task is to determine whether the proposed backstory is consistent with the narrative as a whole.

The objective goes beyond surface-level contradiction detection.
The system must verify:

Consistency over time

Causal coherence

Respect for narrative constraints

Evidence-based reasoning across the text

2. ğŸ§  System Overview

We frame the task as hypothesis verification:

Novel â†’ Ground truth

Backstory â†’ Hypothesis

The system integrates semantic retrieval, claim-level reasoning, and deterministic aggregation to produce a final binary decision.

ğŸ”— High-Level Pipeline
Full Novel
 â†’ Chunking & Indexing (Pathway)
 â†’ Hypothetical Backstory
 â†’ Claim Decomposition
 â†’ Semantic Evidence Retrieval
 â†’ Claim-Level Reasoning (LLM)
 â†’ Deterministic Aggregation
 â†’ Final Binary Decision (results.csv)
 ## ğŸ—ºï¸ System Architecture Diagram
 flowchart TD
    A[Full Novel<br/>(100k+ words)] --> B[Chunking]
    B --> C[Novel Chunks<br/>(chunk_id + text)]
    C --> D[Pathway Index]

    D -->|Semantic Retrieval| G[Relevant Narrative Chunks]

    E[Hypothetical Backstory] --> F[Claim Decomposition]
    F --> H[Atomic Backstory Claims<br/>(core / non-core)]

    H --> I[Claim-Level Reasoning<br/>(LLM)]
    G --> I

    I --> J[Claim Scores<br/>(score, core)]

    J --> K[Deterministic Aggregation]
    K --> L[Final Decision]

    L --> M[results.csv<br/>(story_id, prediction, rationale)]


3. ğŸ“š Long-Context Handling: Novel Chunking & Indexing

The complete novel is processed without truncation, enabling full-context reasoning.

Chunking Strategy

The narrative is divided into overlapping text chunks

Each chunk represents a localized unit (scene / paragraph group)

Every chunk is assigned a unique identifier

This enables:

Scalable processing of long narratives

Targeted evidence retrieval

Preservation of global narrative coherence

4. ğŸ§± Pathway Integration (Track A Requirement)

Pathwayâ€™s Python framework serves as the core infrastructure layer.

Pathway is responsible for:

Ingesting and storing all novel chunks

Generating semantic embeddings

Building a vector index for semantic search

Retrieving narrative passages relevant to backstory claims

ğŸ” Semantic indexing allows retrieval by meaning rather than exact keywords, which is essential for reasoning about character behavior and development over time.

5. ğŸ§¬ Hypothetical Backstory & Claim Decomposition

The hypothetical backstory is provided as an external input and is not generated by the system.

To enable structured evaluation, the backstory is decomposed into atomic, testable claims, such as:

Beliefs

Fears

Motivations

Formative experiences

Worldview assumptions

Each claim is labeled as:

Core â†’ Fundamental to character identity or long-term motivation

Non-core â†’ Secondary or contextual details

This decomposition avoids holistic â€œvibe-basedâ€ judgments and enables precise, interpretable reasoning.

6. ğŸ” Semantic Retrieval of Narrative Evidence

For each backstory claim:

A semantic query is issued to the Pathway index

Multiple relevant narrative chunks are retrieved

Evidence may include:

Actions

Dialogue

Internal monologue

Repeated behavioral patterns

This ensures that decisions are grounded in signals drawn from multiple parts of the text, not isolated passages.

7. ğŸ¤– Claim-Level Consistency Reasoning (LLM)

Each claim is evaluated against the retrieved evidence using a Large Language Model (LLM).

At a high level, the LLM assesses:

Whether the claim is supported, contradicted, or unconstrained

Whether the claim preserves causal consistency with later events

Whether character actions remain coherent over time

Output Encoding

score = 1 â†’ Consistent

score = 0 â†’ Contradictory

âš ï¸ Absence of evidence is not treated as contradiction.

8. ğŸ§® Deterministic Aggregation Logic

Claim-level results are combined using a frozen, deterministic rule.

Aggregation Rule

If any core claim has score = 0 â†’ Contradict (0)

Otherwise â†’ Consistent (1)

This strict rule enforces narrative constraints and prevents critical contradictions from being diluted by majority voting or probabilistic averaging.

9. ğŸ“¥ Input & ğŸ“¤ Output Specification
Input (Placeholder Structure)
{
  "story_XXX": {
    "claims": [
      {
        "claim_id": "claim_1",
        "score": 0 or 1,
        "core": true or false
      }
    ]
  }
}

Output (results.csv)
story_id,prediction,rationale


Field Definitions

story_id â†’ Unique identifier

prediction

1 â†’ Consistent

0 â†’ Contradict

rationale â†’ Short human-readable explanation

Dummy Example (Placeholder)
story_id,prediction,rationale
story_XXX,0,<a core backstory claim contradicts the narrative>
story_YYY,1,<all core backstory claims are consistent>

10. ğŸ Conclusion

This system provides a robust, explainable, and scalable solution for verifying backstoryâ€“narrative consistency in long-form fiction.

By combining:

Pathway-based semantic retrieval

Structured claim-level reasoning

Deterministic aggregation

the pipeline ensures evidence-grounded decisions while effectively handling long-context narratives.